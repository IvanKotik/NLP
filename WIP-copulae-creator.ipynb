{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from itertools import combinations \n",
    "from itertools import permutations\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stopwords \u001b[39m=\u001b[39m english\n",
      "\u001b[0;31mNameError\u001b[0m: name 'english' is not defined"
     ]
    }
   ],
   "source": [
    "stopwords = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseletlist = (\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F175%2F163757167020201210_Liu_crypto_p2p_lending.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F272%2F1654160257Lesson1-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F273%2F1654160288Lesson1-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F274%2F1654160327Lesson1-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F275%2F1654160374Lesson1-4.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F276%2F1654160475Lesson1-5.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F277%2F1654160518Lesson1-6.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F279%2F1654251498Lesson2-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F278%2F1654160549Lesson1-7.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F280%2F1654251511Lesson2-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F281%2F1654251525Lesson2-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F103%2F20210303+IA+METIS+Reinforcement+Learning.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F107%2F1636712642CATE_meets_ML_Presentation.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163458263420190429+Hae+Ni+LDA+DTM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163646337920210921+Hae+Ni+LDA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F121%2F163646358220210708+Hae+Ni+LDA+extensions.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F108%2F163595835420210530+METIS+WANG+Kalman+Filter.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F110%2F1632126441nodalida2021_summaryQuality_slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F112%2F163664661320211013+Ren+LI+Hae+Expectile+FRM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F114%2F1635233254Shapley.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F118%2F1636625638FRM%40EM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F119%2F163231165920210324+Wan+Hae+Li+k-expectile+clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F130%2F1633104997PAC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F134%2F163368764620210923+Mer+Hae+GAN+Generative+Adversarial+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F139%2F163402565120211012+Kho+Hae+Trespassing+random+forests+with+a+pointed+stick+for+self+defence.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F217%2F1644582711Berlin_short_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F235%2F1649426301Variable+importance+measures+for+RF+.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F179%2F16376558632021122+SBA+JW+Hae+EPF++Quantinar.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F186%2F1645194357Presentation_Quantinar_with_videos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F306%2FBarHan2021_talk.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F181%2F163826827620211130+LI+Hae+Case+based+Bancruptcy+prediction.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F224%2F164728525020220305+LI+Electricity+Market+Coupling.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F196%2F1642599075163458186420200403+METIS+Kho+Hae+Spectral+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F197%2F1642599236164171878820211207+Hae+Zin+Hierarchical+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F198%2F1642599289163231165920210324+Wan+Hae+Li+k-expectile+clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F200%2F1642686124163527012720210526+SAE+NAG+HAE+SIZ+Understanding+jumps+in+high+frequency+digital+asset+markets_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F201%2F16426862241636625638FRM%40EM_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F202%2F1642686343163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F189%2F163958119120211130+Hae+Wan+Kot+ComputerMuseum.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F183%2F1643806658KDE+ill-posed+problems.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F167%2F163699399420211115+Liu+Word+Embeddings+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F157%2F1642778303introduction_data_science.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F180%2F16377653596.+model+assessment+-+part+4+-+appendix.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F128%2F163707553520210331_METIS_Hel_GANs_for_Time_Series.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F127%2F163458156320190528+Cea+Hae+Scagnostics.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F117%2F163458186420200403+METIS+Kho+Hae+Spectral+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F106%2F163458175020200914+Hae+DS2+Data+Science+%26+Digital+Society.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F190%2F1640038524Instruction+for+Creating+Quantlets.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F144%2F1636624210NNCSR_Slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F184%2F163888970320211207+Zin+Reu+Hae+USC+Quantinar+40+min+PDF.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F133%2F163707963820210525_Hae_Xia_Crypto_Indices-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F129%2F163458163120200915+Kim+Hae+Tri+VCRIX.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F123%2F163664570620210922+Mat+Pac+Hae+guide+hedging+CC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F122%2F163283000220210923+Cul+Hae+Pet+Xia+Cryptocurrency+as+an+asset+class.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F116%2F163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F113%2F1632580703FRM+for+Cryptos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F101%2F163170747120210914+Reu+DSF+Digital+Surrogate+Finance+Doc.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F97%2F163458105020210808+METIS+Win+Pricing+Kernel+Risk+Premium.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F163299370720210908_CRC21_Hae_Rodeo_or_Ascot.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F1635155323202109_RoA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F79%2F164604019720210502+Hae+Har+Reu+Understanding+CCs.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F241%2F1650632942Biographical+Background+Information.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F210%2F164390414020220130+METIS+Gua+Hae+Model+Selection+Criteria.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F194%2F164171878820211207+Hae+Zin+Hierarchical+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F170%2F163709451820211117+Hae+Qia+Network+Centrality.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F142%2F163627928020211107+Hae+Iva+Mat+Delaunay+Triangulation_A_Shape.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F135%2F1649084960Chapter+1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F136%2F1649094328Chapter+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F137%2F1633950248Chapter+3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F138%2F1649094430Chapter+4.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/ivankotik/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/ivankotik/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stopwords_list \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ids \u001b[39m=\u001b[39m ([\u001b[39m\"\u001b[39m\u001b[39mid-\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(courseletlist))])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mid_name\u001b[39m\u001b[39m'\u001b[39m: ids, \u001b[39m'\u001b[39m\u001b[39murlink\u001b[39m\u001b[39m'\u001b[39m: courseletlist})\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/ivankotik/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "ids = ([\"id-{}\".format(i+1) for i in range(len(courseletlist))])\n",
    "pd.DataFrame({'id_name': ids, 'urlink': courseletlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(file_name, url):\n",
    "    '''Download a PDF file with an URL (Step 1)'''\n",
    "    # Define HTTP Headers\n",
    "    headers = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "    # Download image\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # if response is OK download the PDF and store it, else write the status\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string(file_name):\n",
    "    '''Transform a PDF file to a list of string pages (Step 2)'''\n",
    "    # opening the file\n",
    "    imported_pdf = open(file_name, 'rb')\n",
    "    # convert PDF to readable file\n",
    "    transformed_pdf = PyPDF2.PdfFileReader(imported_pdf)\n",
    "    # get number of pages\n",
    "    totalpages = transformed_pdf.numPages\n",
    "    # read the data and store in a list\n",
    "    pdf_output = [transformed_pdf.getPage(i) for i in range(totalpages)]\n",
    "    # extract result\n",
    "    pdf_output = [pdf_output[i].extractText() for i in range(totalpages)]\n",
    "    return pdf_output, totalpages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(file_name):\n",
    "    '''Initial PDF cleaning procedure (Step 3)'''\n",
    "    pdf_output, totalpages = create_string(file_name)\n",
    "    # cleaning URLs\n",
    "    pdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # cleaning symbols\n",
    "    pdf_output = [re.sub(pattern = \"(\\)|\\(|,|\\.|!|=|:|\\[|\\]|\\{|\\}|\\'|\\\"|#|<|>|\\%|\\&|\\?|\\*|\\/|-|\\$|\\+|\\d)\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # cleaning multispaces\n",
    "    pdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # cleaning out 1-worders\n",
    "    pdf_output = [re.sub(pattern = \" \\w \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # lower-casing\n",
    "    pdf_output = [pdf_output[i].lower() for i in range(totalpages)]\n",
    "    return pdf_output, totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lists(file_name):\n",
    "    '''Creating the base one-word, two-word and three-word lists, the permutation lists for two- and three-word lists (Step 4)'''\n",
    "    pdf_output, totalpages = cleaning(file_name)\n",
    "    # split to a list\n",
    "    word_list = [pdf_output[i].split(\" \") for i in range(totalpages)]\n",
    "    # converting to a dataframe\n",
    "    word_list = pd.DataFrame(word_list)\n",
    "    # one-word section\n",
    "    one_word_list = [word_list.iloc[j, i] for j in range(totalpages) for i in range(len(word_list))]\n",
    "    one_word_list = [x for x in one_word_list if x not in stopwords_list]\n",
    "\n",
    "    # two-word section\n",
    "    two_word_list = [[word_list.iloc[j, i], word_list.iloc[j, i+1]] for j in range(totalpages)  for i in range(len(word_list) - 1)]\n",
    "    two_word_permutation_list = [[p for p in permutations(two_word_list[k])][1:] for k in range(len(two_word_list))]\n",
    "    two_word_permutation_set = set(list(chain(*two_word_permutation_list)))\n",
    "    two_word_permutation_set = pd.DataFrame(two_word_permutation_set)\n",
    "    # three-word section\n",
    "    three_word_list = [[word_list.iloc[j, i], word_list.iloc[j, i+1], word_list.iloc[j, i+2]] for j in range(totalpages) for i in range(len(word_list) - 2)]\n",
    "    three_word_permutation_list = [[p for p in permutations(three_word_list[k])][1:] for k in range(len(three_word_list))]\n",
    "    three_word_permutation_set = set(list(chain(*three_word_permutation_list)))\n",
    "    three_word_permutation_set = pd.DataFrame(three_word_permutation_set)\n",
    "\n",
    "    return word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurrance_three_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrices for the three-word lists (Step 5)'''\n",
    "    word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = three_word_list.copy()\n",
    "    # converting to a dataframe\n",
    "    words = pd.DataFrame(three_word_list)\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [words.iloc[i,0] + \" \" + words.iloc[i,1] + \" \" + words.iloc[i,2] for i in range(len(three_word_list)) if words.iloc[i,].isna().any() == False]\n",
    "    # crating the dictionary\n",
    "    dictionary_three_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_three_word:\n",
    "            dictionary_three_word[word] = dictionary_three_word[word] + 1\n",
    "        else:\n",
    "            dictionary_three_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_three_words = dictionary_three_word.items()\n",
    "    dictionary_three_list = list(dictionary_three_words)\n",
    "    occurrence_three_matrix = pd.DataFrame(dictionary_three_list)\n",
    "    occurrence_three_matrix = occurrence_three_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_three_matrix = occurrence_three_matrix.loc[occurrence_three_matrix.word.isna() == False, ]\n",
    "    occurrence_three_matrix = occurrence_three_matrix.loc[occurrence_three_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_three_matrix = occurrence_three_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_three_matrix['index'] = range(len(occurrence_three_matrix))\n",
    "    occurrence_three_matrix = occurrence_three_matrix.set_index('index')\n",
    "    return occurrence_three_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurrance_two_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrices for the two-word lists (Step 6)'''\n",
    "    word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = two_word_list.copy()\n",
    "    # converting to a dataframe\n",
    "    words = pd.DataFrame(two_word_list)\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [words.iloc[i,0] + \" \" + words.iloc[i,1] for i in range(len(two_word_list)) if words.iloc[i,].isna().any() == False]\n",
    "    # crating the dictionary\n",
    "    dictionary_two_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_two_word:\n",
    "            dictionary_two_word[word] = dictionary_two_word[word] + 1\n",
    "        else:\n",
    "            dictionary_two_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_two_words = dictionary_two_word.items()\n",
    "    dictionary_three_list = list(dictionary_two_words)\n",
    "    occurrence_two_matrix = pd.DataFrame(dictionary_three_list)\n",
    "    occurrence_two_matrix = occurrence_two_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_two_matrix = occurrence_two_matrix.loc[occurrence_two_matrix.word.isna() == False, ]\n",
    "    occurrence_two_matrix = occurrence_two_matrix.loc[occurrence_two_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_two_matrix = occurrence_two_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_two_matrix['index'] = range(len(occurrence_two_matrix))\n",
    "    occurrence_two_matrix = occurrence_two_matrix.set_index('index')\n",
    "    return occurrence_two_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurance_one_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrix for one-word combinations (Step 7)'''\n",
    "    word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = one_word_list.copy()\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [x for x in words if x != \"\"]\n",
    "    words = [x for x in words if x != \" \"]    \n",
    "    # crating the dictionary\n",
    "    dictionary_one_word = dict()\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_one_word:\n",
    "            dictionary_one_word[word] = dictionary_one_word[word] + 1\n",
    "        else:\n",
    "            dictionary_one_word[word] = 1\n",
    "    \n",
    "    # creating the occurance matrix\n",
    "    dictionary_one_word = dictionary_one_word.items()\n",
    "    occurrence_one_matrix = pd.DataFrame(dictionary_one_word)\n",
    "    occurrence_one_matrix = occurrence_one_matrix.rename(columns={0:\"word\", 1:\"occurance\"})\n",
    "    \n",
    "    # clean of NaNs\n",
    "    occurrence_one_matrix = occurrence_one_matrix.loc[occurrence_one_matrix.word.isna() == False, ]\n",
    "    occurrence_one_matrix = occurrence_one_matrix.loc[occurrence_one_matrix.word != \"None\", ]\n",
    "\n",
    "    # sort values\n",
    "    occurrence_one_matrix = occurrence_one_matrix.sort_values(\"occurance\", ascending=False)\n",
    "\n",
    "    # re-indexing\n",
    "    occurrence_one_matrix['index'] = range(len(occurrence_one_matrix))\n",
    "    occurrence_one_matrix = occurrence_one_matrix.set_index('index')\n",
    "    return occurrence_one_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_script(file_name, url):\n",
    "    download_pdf(file_name, url)\n",
    "    word_lists(file_name)\n",
    "    word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    occurrence_three_matrix = occurrance_three_matrix_creator(file_name)\n",
    "    occurrence_two_matrix = occurrance_two_matrix_creator(file_name)\n",
    "    occurrence_one_matrix = occurance_one_matrix_creator(file_name)\n",
    "    os.remove(file_name)\n",
    "    occurrence_one_matrix['id'] = file_name\n",
    "    occurrence_two_matrix['id'] = file_name\n",
    "    occurrence_three_matrix['id'] = file_name\n",
    "    two_word_permutation_set['id'] = file_name\n",
    "    three_word_permutation_set['id'] = file_name\n",
    "    return occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_script(ids[34], courseletlist[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 79 is out of bounds for axis 0 with size 79",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/range.py:929\u001b[0m, in \u001b[0;36mRangeIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range[new_key]\n\u001b[1;32m    930\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m occurrence_one_matrix_z, occurrence_two_matrix_z, occurrence_three_matrix_z, two_word_permutation_set_z, three_word_permutation_set_z \u001b[39m=\u001b[39m main_script(ids[\u001b[39m0\u001b[39m], courseletlist[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(ids)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set \u001b[39m=\u001b[39m main_script(ids[i], courseletlist[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     occurrence_one_matrix_z \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([occurrence_one_matrix_z, occurrence_one_matrix])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     occurrence_two_matrix_z \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 12\u001b[0m in \u001b[0;36mmain_script\u001b[0;34m(file_name, url)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain_script\u001b[39m(file_name, url):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     download_pdf(file_name, url)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     word_lists(file_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     word_list, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set \u001b[39m=\u001b[39m word_lists(file_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     occurrence_three_matrix \u001b[39m=\u001b[39m occurrance_three_matrix_creator(file_name)\n",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 12\u001b[0m in \u001b[0;36mword_lists\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m word_list \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(word_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# one-word section\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m one_word_list \u001b[39m=\u001b[39m [word_list\u001b[39m.\u001b[39miloc[j, i] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(totalpages) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_list))]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# two-word section\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m two_word_list \u001b[39m=\u001b[39m [[word_list\u001b[39m.\u001b[39miloc[j, i], word_list\u001b[39m.\u001b[39miloc[j, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(totalpages)  \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]\n",
      "\u001b[1;32m/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb Cell 12\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m word_list \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(word_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# one-word section\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m one_word_list \u001b[39m=\u001b[39m [word_list\u001b[39m.\u001b[39;49miloc[j, i] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(totalpages) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_list))]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# two-word section\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivankotik/Documents/NPL/Word-cloud-Search-engine-optimisation-/WIP-copulae-creator.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m two_word_list \u001b[39m=\u001b[39m [[word_list\u001b[39m.\u001b[39miloc[j, i], word_list\u001b[39m.\u001b[39miloc[j, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(totalpages)  \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexing.py:960\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    958\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[1;32m    959\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m--> 960\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[1;32m    961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3612\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3593\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3594\u001b[0m \u001b[39mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[1;32m   3595\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3609\u001b[0m \u001b[39m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[1;32m   3610\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3611\u001b[0m \u001b[39mif\u001b[39;00m takeable:\n\u001b[0;32m-> 3612\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ixs(col, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   3613\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m   3615\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3439\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3435\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m   3437\u001b[0m \u001b[39m# icol\u001b[39;00m\n\u001b[1;32m   3438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3439\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns[i]\n\u001b[1;32m   3441\u001b[0m     col_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39miget(i)\n\u001b[1;32m   3442\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_box_col_values(col_mgr, i)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/range.py:931\u001b[0m, in \u001b[0;36mRangeIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range[new_key]\n\u001b[1;32m    930\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for axis 0 with size \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39melif\u001b[39;00m is_scalar(key):\n\u001b[1;32m    935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39monly integers, slices (`:`), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mellipsis (`...`), numpy.newaxis (`None`) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand integer or boolean \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marrays are valid indices\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    940\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 79 is out of bounds for axis 0 with size 79"
     ]
    }
   ],
   "source": [
    "# occurrence_one_matrix_z, occurrence_two_matrix_z, occurrence_three_matrix_z, two_word_permutation_set_z, three_word_permutation_set_z = main_script(ids[0], courseletlist[0])\n",
    "# for i in range(1, len(ids)+1):\n",
    "#     occurrence_one_matrix, occurrence_two_matrix, occurrence_three_matrix, two_word_permutation_set, three_word_permutation_set = main_script(ids[i], courseletlist[i])\n",
    "#     occurrence_one_matrix_z = pd.concat([occurrence_one_matrix_z, occurrence_one_matrix])\n",
    "#     occurrence_two_matrix_z = pd.concat([occurrence_two_matrix_z, occurrence_two_matrix])\n",
    "#     occurrence_three_matrix_z = pd.concat([occurrence_three_matrix_z, occurrence_three_matrix])\n",
    "#     two_word_permutation_set_z = pd.concat([two_word_permutation_set_z, two_word_permutation_set])\n",
    "#     three_word_permutation_set_z = pd.concat([three_word_permutation_set_z, three_word_permutation_set])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
