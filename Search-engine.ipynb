{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search term:  ['price', 'hedg', 'invers', 'btc', 'option']\n",
      "search result: present\n",
      "\n",
      "{\"0\":{\"id\":282},\"1\":{\"id\":79},\"2\":{\"id\":266},\"3\":{\"id\":239},\"4\":{\"id\":240}}\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "# search terms\n",
    "input_initial = 'Pricing and hedging inverse BTC options'\n",
    "number_of_urls = 5\n",
    "\n",
    "# fetching all pdfs\n",
    "url_text = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/419447491efef2bb3a21b0459e5bdcd352a39097/combined_pdf_json.json'\n",
    "r = requests.get(url_text)\n",
    "combined_pdf = r.json()\n",
    "\n",
    "\n",
    "# fetching master list\n",
    "url_master = 'https://raw.githubusercontent.com/IvanKotik/Word-cloud-Search-engine-optimisation-/master/q-master-json.json'\n",
    "e = requests.get(url_master)\n",
    "q_master_json = e.json()\n",
    "\n",
    "\n",
    "# dataframing master list\n",
    "q_master = pd.DataFrame({'id' : [q_master_json[str(i)]['id'] for i in range(len(q_master_json))],\n",
    "'name' : [q_master_json[str(i)]['name'] for i in range(len(q_master_json))],\n",
    "'team' : [q_master_json[str(i)]['team'] for i in range(len(q_master_json))],\n",
    "'artist' : [q_master_json[str(i)]['artist'] for i in range(len(q_master_json))],\n",
    "'author' : [q_master_json[str(i)]['author'] for i in range(len(q_master_json))],\n",
    "'published_in' : [q_master_json[str(i)]['published_in'] for i in range(len(q_master_json))],\n",
    "'full_link' : [q_master_json[str(i)]['full_link'] for i in range(len(q_master_json))],\n",
    "'pdf_url' : [q_master_json[str(i)]['pdf_url'] for i in range(len(q_master_json))]\n",
    "})\n",
    "q_master['url_check'] = [len(i) for i in q_master['pdf_url']]\n",
    "q_master = q_master.loc[q_master['url_check'] != 0, ]\n",
    "q_master = q_master.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# fetching fresh master\n",
    "url_fresh_master = 'https://quantinar.com/api/flower/index'\n",
    "t = requests.get(url_fresh_master)\n",
    "q_fresh_json = t.json()\n",
    "\n",
    "\n",
    "# fresh master dataframing\n",
    "q_check = pd.DataFrame({'id' : [q_fresh_json['data'][i]['id'] for i in range(len(q_fresh_json['data']))],\n",
    "'name' : [q_fresh_json['data'][i]['name'] for i in range(len(q_fresh_json['data']))],\n",
    "'team' : [q_fresh_json['data'][i]['team'] for i in range(len(q_fresh_json['data']))],\n",
    "'artist' : [q_fresh_json['data'][i]['artist'] for i in range(len(q_fresh_json['data']))],\n",
    "'author' : [q_fresh_json['data'][i]['author'] for i in range(len(q_fresh_json['data']))],\n",
    "'published_in' : [q_fresh_json['data'][i]['published_in'] for i in range(len(q_fresh_json['data']))],\n",
    "'full_link' : [q_fresh_json['data'][i]['full_link'] for i in range(len(q_fresh_json['data']))],\n",
    "'pdf_url' : [q_fresh_json['data'][i]['pdf_url'] for i in range(len(q_fresh_json['data']))]\n",
    "})\n",
    "q_check['url_check'] = [len(i) for i in q_check['pdf_url']]\n",
    "q_check = q_check.loc[q_check['url_check'] != 0, ]\n",
    "q_check = q_check.reset_index(drop=True)\n",
    "\n",
    "\n",
    "stopwords_list = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",'that',\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"}\n",
    "\n",
    "\n",
    "def download_pdf(file_name, url):\n",
    "\n",
    "    '''Download a PDF file with an URL'''\n",
    "\n",
    "    # Define HTTP Headers\n",
    "    headers = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "    \n",
    "    # Download image\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # response = requests.get(url)\n",
    "    \n",
    "    # if response is OK download the PDF and store it, else write the status\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def create_string(file_name, url):\n",
    "    \n",
    "    download_pdf(file_name, url)\n",
    "\n",
    "    '''Transform a PDF file to a list of string pages'''\n",
    "    \n",
    "    # opening the file\n",
    "    imported_pdf = open(file_name, 'rb')\n",
    "    \n",
    "    # convert PDF to readable file\n",
    "    transformed_pdf = PyPDF2.PdfFileReader(imported_pdf, strict=False)\n",
    "    \n",
    "    # get number of pages\n",
    "    totalpages = transformed_pdf.numPages\n",
    "    \n",
    "    # read the data and store in a list\n",
    "    pdf_output = [transformed_pdf.getPage(i) for i in range(totalpages)]\n",
    "\n",
    "    # extract result\n",
    "    pdf_output = [pdf_output[i].extractText() for i in range(totalpages)]\n",
    "    \n",
    "    return pdf_output, totalpages \n",
    "\n",
    "\n",
    "def cleaning(file_name, url):\n",
    "\n",
    "    '''Initial PDF cleaning procedure'''\n",
    "    \n",
    "    pdf_output, totalpages = create_string(file_name, url)\n",
    "    \n",
    "    # # cleaning URLs\n",
    "    pdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    \n",
    "    # # cleaning symbols\n",
    "    pdf_output = [re.sub(pattern = \"\\\\n\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"\\W|\\d\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"[^a-zA-Z]\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    \n",
    "    # # cleaning multispaces\n",
    "    pdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    \n",
    "    # # cleaning out 1-2-worders\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    \n",
    "    # # lower-casing\n",
    "    pdf_output = [pdf_output[i].lower() for i in range(totalpages)]\n",
    "    pdf_output = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in pdf_output]\n",
    "    pdf_output = [' '.join(pdf_output[i]) for i in range(len(pdf_output))]\n",
    "    \n",
    "    return pdf_output, totalpages\n",
    "\n",
    "\n",
    "def combined_pdf_creator():\n",
    "    '''Creating the final master-pdf dataframe'''\n",
    "\n",
    "    # clean the first pdf\n",
    "    pdf_output, totalpages = cleaning(str(q_master.iloc[0, 0]), q_master.iloc[0 ,7])\n",
    "\n",
    "    # combine the pdf\n",
    "    combined_pdf = [' '.join(pdf_output)]\n",
    "\n",
    "    # iterate on above\n",
    "    for i in range(1, q_master.shape[0]):\n",
    "        print(i)\n",
    "        t = time.process_time()\n",
    "        try:\n",
    "            pdf_output, totalpages = cleaning(str(q_master.iloc[i, 0]), q_master.iloc[i ,7])\n",
    "            combined_pdf.append(' '.join(pdf_output))\n",
    "        except:\n",
    "            print('problematic file: ', str(q_master.iloc[i, 0]), q_master.iloc[i ,7])\n",
    "            combined_pdf.append(' '.join(''))\n",
    "        finally:\n",
    "            print('time elapsed: ', (time.process_time() - t))\n",
    "    return combined_pdf\n",
    "\n",
    "\n",
    "# if triggered, then it means that the pdf downloading must happen again\n",
    "if all([any(o == q_master['id']) for o in [i for i in q_check['id']]]) == False:\n",
    "    combined_pdf = combined_pdf_creator()\n",
    "    combined_pdf_df = pd.DataFrame({'id' : q_check['id'], \"text\" : combined_pdf})\n",
    "    combined_pdf_json = combined_pdf_df.to_json(orient='index')\n",
    "    with open(\"combined_pdf_json.json\", \"w\") as outfile:\n",
    "        outfile.write(combined_pdf_json)\n",
    "else: \n",
    "    combined_pdf = [combined_pdf[str(i)]['text'] for i in range(len(combined_pdf))]\n",
    "\n",
    "\n",
    "def input_sequence(input_initial): \n",
    "\n",
    "    '''Trimming input search terms to be used for the occurrence matrix. The output is a generalized stemmed input form ready for checking and a count of terms for the ngram_range.'''\n",
    "\n",
    "    # splitting the phrase by pieces\n",
    "    input_general = input_initial.split(' ')\n",
    "\n",
    "    # cleaning stopwords\n",
    "    input_general = [i for i in input_general if i not in stopwords_list]\n",
    "\n",
    "    # count words\n",
    "    input_general_count = len(input_general)\n",
    "\n",
    "    # stem the words\n",
    "    input_general = [ps.stem(i) for i in input_general]\n",
    "\n",
    "    # create the additional variations of the phrase\n",
    "    outer_list = []\n",
    "    for i in range(0, input_general_count):\n",
    "        inner_list = [input_general[j : input_general_count-i+j] for j in range(i+1)]\n",
    "        outer_list.append(inner_list)\n",
    "\n",
    "    return input_general, input_general_count, outer_list\n",
    "\n",
    "\n",
    "def general_occurrence(input_general_count, combined_pdf): \n",
    "\n",
    "    '''Creation of the generalized tfidf occurance matrix based on dynamic parameters.'''\n",
    "\n",
    "    vectorizer_general = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase=False, stop_words=stopwords_list, ngram_range=(input_general_count, input_general_count))\n",
    "    X_general = vectorizer_general.fit_transform(combined_pdf)\n",
    "    xx_general = pd.DataFrame(X_general.toarray(), columns = vectorizer_general.get_feature_names_out())\n",
    "    return xx_general\n",
    "\n",
    "def check_for_general(input_initial, input_general_count, outer_list, combined_pdf, number_of_urls):\n",
    "\n",
    "    '''Main function.'''\n",
    "\n",
    "    # initiating a breaker function\n",
    "    breaker = 0\n",
    "    # creating the occurrence matrix for max length\n",
    "    xx_general = general_occurrence(input_general_count, combined_pdf)\n",
    "    # creating an empty table for results\n",
    "    test_output = xx_general.copy()\n",
    "    test_output = test_output.iloc[:,0]*0\n",
    "    # first test for full match\n",
    "    print('search term: ', outer_list[0][0])\n",
    "    test = ' '.join(outer_list[0][0])\n",
    "    # if test passed\n",
    "    if test in list(xx_general.columns):\n",
    "        # create a ranked index\n",
    "        ranked_indexes = xx_general[test].sort_values(ascending=False).index\n",
    "        ranked_indexes = list(ranked_indexes[0:number_of_urls])\n",
    "        # connect back to urls\n",
    "        output_url = [q_master['id'][i] for i in ranked_indexes]\n",
    "        print('search result: present\\n')\n",
    "        return output_url\n",
    "    # if test failed drill-down\n",
    "    else: \n",
    "        print('search result: not present, drill-down\\n')\n",
    "        for y in range(1, len(outer_list)):\n",
    "            # create a new occurance matrix with new ngrams\n",
    "            xx_general = general_occurrence(input_general_count-y, combined_pdf)\n",
    "            for u in range(y+1):\n",
    "                # drill-down phrase test\n",
    "                print('search term: ', outer_list[y][u])\n",
    "                test = ' '.join(outer_list[y][u])\n",
    "                # if test passed\n",
    "                if test in list(xx_general.columns):\n",
    "                    # sum the tfidf indexes across multiple matches\n",
    "                    test_output += xx_general[test]\n",
    "                    print('search result: present\\n')\n",
    "                    # initiate the exit from the function\n",
    "                    breaker = 1\n",
    "                else: \n",
    "                    print('search result: not present\\n')\n",
    "            if breaker == 1:\n",
    "                # order the indexes by highest tfidf\n",
    "                ranked_indexes = test_output.sort_values(ascending=False).index\n",
    "                ranked_indexes = list(ranked_indexes[0:number_of_urls])\n",
    "                # return urls\n",
    "                output_url = [q_master['id'][i] for i in ranked_indexes]\n",
    "                return output_url\n",
    "\n",
    "\n",
    "input_general, input_general_count, outer_list = input_sequence(input_initial)\n",
    "output_url = check_for_general(input_initial, input_general_count, outer_list, combined_pdf, number_of_urls)\n",
    "json_output = pd.DataFrame({'id': output_url}).to_json(orient='index')\n",
    "print(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
